{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "The purpose of this notebook is perform feature selection. First we will have to sample the data to 10% of the original observations. This is necessary because the large size of the full feature matrices does not allow for efficient feature selection or for a significant number of random search iterations.\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "1. Sample 10% of the training observations randomly\n",
    "2. Convert numeric columns to `np.float32`\n",
    "3. Convert boolean columns to `np.uint8`\n",
    "4. One-hot encode categorical features as necessary\n",
    "5. Remove one of every pair of columns with all duplicated values (1.0 correlation)\n",
    "6. Remove columns with more than 90% missing values\n",
    "7. Remove columns with a single unique value\n",
    "8. Remove one of every pair of columns with abs(correlation) > 0.95\n",
    "\n",
    "The reduced data set will then be saved as `_sample.csv` and can be used for 100 iterations of random search with the Gradient Boosting Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuretools Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = pd.read_csv('../input/feature_matrix.csv', low_memory=False)\n",
    "\n",
    "# Sampling 10% of the original data\n",
    "train = feature_matrix[feature_matrix['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "del feature_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE) not in data\n",
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Closed) not in data\n",
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Active) not in data\n",
      "SUM(bureau_balance.bureau.PREVIOUS_OTHER_LOAN_RATE) not in data\n"
     ]
    }
   ],
   "source": [
    "for col in ['SUM(bureau.PREVIOUS_OTHER_LOAN_RATE)', 'SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Closed)',\n",
    "            'SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Active)', 'SUM(bureau_balance.bureau.PREVIOUS_OTHER_LOAN_RATE)']:\n",
    "    try:\n",
    "        train[col] = train[col].astype(np.float32)\n",
    "    except:\n",
    "        print(f'{col} not in data')\n",
    "    \n",
    "for col in train:\n",
    "    if train[col].dtype == 'bool':\n",
    "        train[col] = train[col].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 2091)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.get_dummies(train)\n",
    "n_features_start = train.shape[1] - 2\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns with duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1816)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, idx, inv, counts = np.unique(train, axis = 1, return_index = True, return_inverse=True, return_counts=True)\n",
    "train = train.iloc[:, idx]\n",
    "n_non_unique_columns = n_features_start - train.shape[1] - 2\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "The threshold is currently set at 90% but could be lowered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1798)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_threshold = 90\n",
    "\n",
    "# Find missing and percentage\n",
    "missing = pd.DataFrame(train.isnull().sum())\n",
    "missing['percent'] = 100 * (missing[0] / train.shape[0])\n",
    "missing.sort_values('percent', ascending = False, inplace = True)\n",
    "\n",
    "# Missing above threshold\n",
    "missing_cols = list(missing[missing['percent'] > missing_threshold].index)\n",
    "n_missing_cols = len(missing_cols)\n",
    "\n",
    "train = train[[x for x in train if x not in missing_cols]]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero variance columns\n",
    "\n",
    "These are any columns with only a single unique value. (`np.nan` does not count as a unique value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1771)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counts = pd.DataFrame(train.nunique()).sort_values(0, ascending = True)\n",
    "zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "train = train[[x for x in train if x not in zero_variance_cols]]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns containing derivations of target\n",
    "\n",
    "This is a special consideration we have to take with the Featuretools data because one of the columns is derived from the `TARGET`. (PERCENTILE transformation works on numeric columns of the data and I accidentally left in the `TARGET` when running the feature synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET\n",
      "PERCENTILE(TARGET)\n"
     ]
    }
   ],
   "source": [
    "for col in train:\n",
    "    if 'TARGET' in col:\n",
    "        print(col)\n",
    "        \n",
    "train.drop(columns = 'PERCENTILE(TARGET)', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Collinear Variables with Correlation Threshold\n",
    "\n",
    "The correlation threshold will be set at 0.95 and one out of every pair of columns that are above this threshold will be removed. The column removed is the one that occurs last in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.95\n",
    "\n",
    "corr_matrix = train.corr()\n",
    "\n",
    "# Extract the upper triangle of the correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# Select the features with correlations above the threshold\n",
    "# Need to use the absolute value\n",
    "to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1042)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[[x for x in train if x not in to_drop]]\n",
    "n_collinear = len(to_drop)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_non_unique_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_zero_variance_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_collinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns removed:  1045\n"
     ]
    }
   ],
   "source": [
    "total_removed = n_non_unique_columns + n_missing_cols + n_zero_variance_cols + n_collinear + 1\n",
    "print('Total columns removed: ', total_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>SUM(installments.DAYS_ENTRY_PAYMENT)</th>\n",
       "      <th>MIN(previous.SUM(installments.DAYS_ENTRY_PAYMENT))</th>\n",
       "      <th>MEAN(previous.SUM(installments.DAYS_ENTRY_PAYMENT))</th>\n",
       "      <th>MAX(previous.SUM(installments.DAYS_ENTRY_PAYMENT))</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>SUM(previous.DAYS_DECISION)</th>\n",
       "      <th>MIN(previous.DAYS_DECISION)</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENTILE(MEAN(bureau.AMT_ANNUITY))</th>\n",
       "      <th>MIN(bureau.PERCENTILE(AMT_ANNUITY))</th>\n",
       "      <th>MEAN(bureau.PERCENTILE(AMT_ANNUITY))</th>\n",
       "      <th>MIN(bureau.NUM_UNIQUE(bureau_balance.STATUS))</th>\n",
       "      <th>MEAN(bureau.NUM_UNIQUE(bureau_balance.STATUS))</th>\n",
       "      <th>MAX(bureau.NUM_UNIQUE(bureau_balance.STATUS))</th>\n",
       "      <th>NUM_UNIQUE(bureau_balance.STATUS)</th>\n",
       "      <th>MIN(bureau.AMT_ANNUITY)</th>\n",
       "      <th>MEAN(bureau.AMT_ANNUITY)</th>\n",
       "      <th>MAX(bureau.AMT_ANNUITY)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83147</th>\n",
       "      <td>-11819.0</td>\n",
       "      <td>-4187.0</td>\n",
       "      <td>-726.0</td>\n",
       "      <td>-726.0</td>\n",
       "      <td>-726.000000</td>\n",
       "      <td>-726.0</td>\n",
       "      <td>-673.0</td>\n",
       "      <td>-601.0</td>\n",
       "      <td>-217.0</td>\n",
       "      <td>-217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354935</th>\n",
       "      <td>-18794.0</td>\n",
       "      <td>-6010.0</td>\n",
       "      <td>-157073.0</td>\n",
       "      <td>-122152.0</td>\n",
       "      <td>-39268.250000</td>\n",
       "      <td>-1026.0</td>\n",
       "      <td>-8164.0</td>\n",
       "      <td>-2201.0</td>\n",
       "      <td>-8634.0</td>\n",
       "      <td>-2648.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64916</th>\n",
       "      <td>-20741.0</td>\n",
       "      <td>-1882.0</td>\n",
       "      <td>-20122.0</td>\n",
       "      <td>-13200.0</td>\n",
       "      <td>-10061.000000</td>\n",
       "      <td>-6922.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4296.0</td>\n",
       "      <td>-2900.0</td>\n",
       "      <td>-2041.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87122</th>\n",
       "      <td>-9881.0</td>\n",
       "      <td>-932.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-185.0</td>\n",
       "      <td>-1076.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280351</th>\n",
       "      <td>-14762.0</td>\n",
       "      <td>-6189.0</td>\n",
       "      <td>-57878.0</td>\n",
       "      <td>-28635.0</td>\n",
       "      <td>-19292.666667</td>\n",
       "      <td>-10697.0</td>\n",
       "      <td>-2370.0</td>\n",
       "      <td>-4187.0</td>\n",
       "      <td>-7679.0</td>\n",
       "      <td>-2745.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661964</td>\n",
       "      <td>0.706131</td>\n",
       "      <td>0.706131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10984.5</td>\n",
       "      <td>10984.5</td>\n",
       "      <td>10984.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1042 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DAYS_BIRTH  DAYS_REGISTRATION  SUM(installments.DAYS_ENTRY_PAYMENT)  \\\n",
       "83147     -11819.0            -4187.0                                -726.0   \n",
       "354935    -18794.0            -6010.0                             -157073.0   \n",
       "64916     -20741.0            -1882.0                              -20122.0   \n",
       "87122      -9881.0             -932.0                                   NaN   \n",
       "280351    -14762.0            -6189.0                              -57878.0   \n",
       "\n",
       "        MIN(previous.SUM(installments.DAYS_ENTRY_PAYMENT))  \\\n",
       "83147                                              -726.0    \n",
       "354935                                          -122152.0    \n",
       "64916                                            -13200.0    \n",
       "87122                                                 NaN    \n",
       "280351                                           -28635.0    \n",
       "\n",
       "        MEAN(previous.SUM(installments.DAYS_ENTRY_PAYMENT))  \\\n",
       "83147                                         -726.000000     \n",
       "354935                                      -39268.250000     \n",
       "64916                                       -10061.000000     \n",
       "87122                                                 NaN     \n",
       "280351                                      -19292.666667     \n",
       "\n",
       "        MAX(previous.SUM(installments.DAYS_ENTRY_PAYMENT))  DAYS_EMPLOYED  \\\n",
       "83147                                              -726.0          -673.0   \n",
       "354935                                            -1026.0         -8164.0   \n",
       "64916                                             -6922.0             NaN   \n",
       "87122                                                 NaN          -185.0   \n",
       "280351                                           -10697.0         -2370.0   \n",
       "\n",
       "        DAYS_ID_PUBLISH  SUM(previous.DAYS_DECISION)  \\\n",
       "83147            -601.0                       -217.0   \n",
       "354935          -2201.0                      -8634.0   \n",
       "64916           -4296.0                      -2900.0   \n",
       "87122           -1076.0                          NaN   \n",
       "280351          -4187.0                      -7679.0   \n",
       "\n",
       "        MIN(previous.DAYS_DECISION)           ...             \\\n",
       "83147                        -217.0           ...              \n",
       "354935                      -2648.0           ...              \n",
       "64916                       -2041.0           ...              \n",
       "87122                           NaN           ...              \n",
       "280351                      -2745.0           ...              \n",
       "\n",
       "        PERCENTILE(MEAN(bureau.AMT_ANNUITY))  \\\n",
       "83147                                    NaN   \n",
       "354935                                   NaN   \n",
       "64916                                    NaN   \n",
       "87122                                    NaN   \n",
       "280351                              0.661964   \n",
       "\n",
       "        MIN(bureau.PERCENTILE(AMT_ANNUITY))  \\\n",
       "83147                                   NaN   \n",
       "354935                                  NaN   \n",
       "64916                                   NaN   \n",
       "87122                                   NaN   \n",
       "280351                             0.706131   \n",
       "\n",
       "        MEAN(bureau.PERCENTILE(AMT_ANNUITY))  \\\n",
       "83147                                    NaN   \n",
       "354935                                   NaN   \n",
       "64916                                    NaN   \n",
       "87122                                    NaN   \n",
       "280351                              0.706131   \n",
       "\n",
       "        MIN(bureau.NUM_UNIQUE(bureau_balance.STATUS))  \\\n",
       "83147                                             NaN   \n",
       "354935                                            NaN   \n",
       "64916                                             NaN   \n",
       "87122                                             NaN   \n",
       "280351                                            3.0   \n",
       "\n",
       "        MEAN(bureau.NUM_UNIQUE(bureau_balance.STATUS))  \\\n",
       "83147                                              NaN   \n",
       "354935                                             NaN   \n",
       "64916                                              NaN   \n",
       "87122                                              NaN   \n",
       "280351                                             3.0   \n",
       "\n",
       "        MAX(bureau.NUM_UNIQUE(bureau_balance.STATUS))  \\\n",
       "83147                                             NaN   \n",
       "354935                                            NaN   \n",
       "64916                                             NaN   \n",
       "87122                                             NaN   \n",
       "280351                                            3.0   \n",
       "\n",
       "        NUM_UNIQUE(bureau_balance.STATUS)  MIN(bureau.AMT_ANNUITY)  \\\n",
       "83147                                 NaN                      NaN   \n",
       "354935                                NaN                      NaN   \n",
       "64916                                 NaN                      NaN   \n",
       "87122                                 NaN                      NaN   \n",
       "280351                                3.0                  10984.5   \n",
       "\n",
       "        MEAN(bureau.AMT_ANNUITY)  MAX(bureau.AMT_ANNUITY)  \n",
       "83147                        NaN                      NaN  \n",
       "354935                       NaN                      NaN  \n",
       "64916                        NaN                      NaN  \n",
       "87122                        NaN                      NaN  \n",
       "280351                   10984.5                  10984.5  \n",
       "\n",
       "[5 rows x 1042 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save sample of Data\n",
    "\n",
    "The resulting data has 10% of the training observations and 1042 columns (1040 of which are features). This data is now ready for random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../input/feature_matrix_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Feature Selection\n",
    "\n",
    "We can refactor the four steps completed above into a single function that applies them in the same sequence to any dataframe. This function will be used on the manual and semi-automated feaures with the same inputs as with the Featuretools features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(feature_matrix, missing_threshold=90, correlation_threshold=0.95):\n",
    "    \"\"\"Feature selection for a dataframe.\"\"\"\n",
    "    \n",
    "    feature_matrix = pd.get_dummies(feature_matrix)\n",
    "    n_features_start = feature_matrix.shape[1]\n",
    "    print('Original shape: ', feature_matrix.shape)\n",
    "\n",
    "    _, idx = np.unique(feature_matrix, axis = 1, return_index = True)\n",
    "    feature_matrix = feature_matrix.iloc[:, idx]\n",
    "    n_non_unique_columns = n_features_start - feature_matrix.shape[1]\n",
    "    print('{}  non-unique valued columns.'.format(n_non_unique_columns))\n",
    "\n",
    "    # Find missing and percentage\n",
    "    missing = pd.DataFrame(feature_matrix.isnull().sum())\n",
    "    missing['percent'] = 100 * (missing[0] / feature_matrix.shape[0])\n",
    "    missing.sort_values('percent', ascending = False, inplace = True)\n",
    "\n",
    "    # Missing above threshold\n",
    "    missing_cols = list(missing[missing['percent'] > missing_threshold].index)\n",
    "    n_missing_cols = len(missing_cols)\n",
    "\n",
    "    # Remove missing columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in missing_cols]]\n",
    "    print('{} missing columns with threshold: {}.'.format(n_missing_cols,\n",
    "                                                                        missing_threshold))\n",
    "    \n",
    "    # Zero variance\n",
    "    unique_counts = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending = True)\n",
    "    zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "    n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "    # Remove zero variance columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in zero_variance_cols]]\n",
    "    print('{} zero variance columns.'.format(n_zero_variance_cols))\n",
    "    \n",
    "    # Correlations\n",
    "    corr_matrix = feature_matrix.corr()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "    # Select the features with correlations above the threshold\n",
    "    # Need to use the absolute value\n",
    "    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "    n_collinear = len(to_drop)\n",
    "    \n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]\n",
    "    print('{} collinear columns removed with threshold: {}.'.format(n_collinear,\n",
    "                                                                          correlation_threshold))\n",
    "    \n",
    "    total_removed = n_non_unique_columns + n_missing_cols + n_zero_variance_cols + n_collinear\n",
    "    \n",
    "    print('Total columns removed: ', total_removed)\n",
    "    print('Shape after feature selection: {}.'.format(feature_matrix.shape))\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Features\n",
    "\n",
    "The process is the same except now we have a function to carry out the feature selection. First we subset to 10% of the training data, and then we apply feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:  (30751, 273)\n",
      "0  non-unique valued columns.\n",
      "2 missing columns with threshold: 90.\n",
      "2 zero variance columns.\n",
      "38 collinear columns removed with threshold: 0.95.\n",
      "Total columns removed:  42\n",
      "Shape after feature selection: (30751, 231).\n"
     ]
    }
   ],
   "source": [
    "manual_features = pd.read_csv('../input/features_manual.csv')\n",
    "manual_features = manual_features[manual_features['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "manual_features = feature_selection(manual_features, 90, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>PREVIOUS_LOAN_DIFFERENCE_MEAN</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>CASH_AVERAGE_LOAN_LENGTH</th>\n",
       "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
       "      <th>FLAG_DOCUMENT_12</th>\n",
       "      <th>FLAG_DOCUMENT_2</th>\n",
       "      <th>ORGANIZATION_TYPE_Trade: type 5</th>\n",
       "      <th>...</th>\n",
       "      <th>ELEVATORS_MODE</th>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>NONLIVINGAREA_MODE</th>\n",
       "      <th>LANDAREA_MODE</th>\n",
       "      <th>LIVINGAPARTMENTS_MODE</th>\n",
       "      <th>FLOORSMIN_AVG</th>\n",
       "      <th>YEARS_BUILD_AVG</th>\n",
       "      <th>CREDIT_CARD_AMT_BALANCE_MEAN_MEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77158</th>\n",
       "      <td>-14017.0</td>\n",
       "      <td>-3747.0</td>\n",
       "      <td>-2475.0000</td>\n",
       "      <td>-2384.0</td>\n",
       "      <td>-1046.0</td>\n",
       "      <td>-78.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306191</th>\n",
       "      <td>-16520.0</td>\n",
       "      <td>-4275.0</td>\n",
       "      <td>3907.5000</td>\n",
       "      <td>-3198.0</td>\n",
       "      <td>-82.0</td>\n",
       "      <td>-57.372263</td>\n",
       "      <td>-1542.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64916</th>\n",
       "      <td>-20741.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7821.0000</td>\n",
       "      <td>-1882.0</td>\n",
       "      <td>-4296.0</td>\n",
       "      <td>-51.678571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81133</th>\n",
       "      <td>-9685.0</td>\n",
       "      <td>-318.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-378.0</td>\n",
       "      <td>-1763.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1090.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231607</th>\n",
       "      <td>-20891.0</td>\n",
       "      <td>-413.0</td>\n",
       "      <td>20321.4375</td>\n",
       "      <td>-3154.0</td>\n",
       "      <td>-3595.0</td>\n",
       "      <td>-38.400000</td>\n",
       "      <td>-1696.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DAYS_BIRTH  DAYS_EMPLOYED  PREVIOUS_LOAN_DIFFERENCE_MEAN  \\\n",
       "77158     -14017.0        -3747.0                     -2475.0000   \n",
       "306191    -16520.0        -4275.0                      3907.5000   \n",
       "64916     -20741.0            NaN                     -7821.0000   \n",
       "81133      -9685.0         -318.0                            NaN   \n",
       "231607    -20891.0         -413.0                     20321.4375   \n",
       "\n",
       "        DAYS_REGISTRATION  DAYS_ID_PUBLISH  CASH_AVERAGE_LOAN_LENGTH  \\\n",
       "77158             -2384.0          -1046.0                -78.000000   \n",
       "306191            -3198.0            -82.0                -57.372263   \n",
       "64916             -1882.0          -4296.0                -51.678571   \n",
       "81133              -378.0          -1763.0                       NaN   \n",
       "231607            -3154.0          -3595.0                -38.400000   \n",
       "\n",
       "        DAYS_LAST_PHONE_CHANGE  FLAG_DOCUMENT_12  FLAG_DOCUMENT_2  \\\n",
       "77158                      0.0               0.0              0.0   \n",
       "306191                 -1542.0               0.0              0.0   \n",
       "64916                      0.0               0.0              0.0   \n",
       "81133                  -1090.0               0.0              0.0   \n",
       "231607                 -1696.0               0.0              0.0   \n",
       "\n",
       "        ORGANIZATION_TYPE_Trade: type 5                ...                  \\\n",
       "77158                                 0                ...                   \n",
       "306191                                0                ...                   \n",
       "64916                                 0                ...                   \n",
       "81133                                 0                ...                   \n",
       "231607                                0                ...                   \n",
       "\n",
       "        ELEVATORS_MODE  COMMONAREA_MODE  NONLIVINGAPARTMENTS_MODE  \\\n",
       "77158              NaN              NaN                       NaN   \n",
       "306191             NaN              NaN                       NaN   \n",
       "64916              NaN              NaN                       NaN   \n",
       "81133              NaN              NaN                       NaN   \n",
       "231607             NaN              NaN                       NaN   \n",
       "\n",
       "        NONLIVINGAPARTMENTS_MEDI  NONLIVINGAREA_MODE  LANDAREA_MODE  \\\n",
       "77158                        NaN                 NaN            NaN   \n",
       "306191                       NaN                 NaN            NaN   \n",
       "64916                        NaN                 NaN            NaN   \n",
       "81133                        NaN                 NaN            NaN   \n",
       "231607                       NaN                 NaN            NaN   \n",
       "\n",
       "        LIVINGAPARTMENTS_MODE  FLOORSMIN_AVG  YEARS_BUILD_AVG  \\\n",
       "77158                     NaN            NaN              NaN   \n",
       "306191                    NaN            NaN              NaN   \n",
       "64916                     NaN            NaN              NaN   \n",
       "81133                     NaN            NaN              NaN   \n",
       "231607                    NaN            NaN              NaN   \n",
       "\n",
       "        CREDIT_CARD_AMT_BALANCE_MEAN_MEAN  \n",
       "77158                                 NaN  \n",
       "306191                                NaN  \n",
       "64916                                 NaN  \n",
       "81133                                 NaN  \n",
       "231607                                NaN  \n",
       "\n",
       "[5 rows x 231 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can save the features for random search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features.to_csv('../input/features_manual_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Automated Features\n",
    "\n",
    "The final feature matrix is that created by what we called __semi-automated__ feature engineering. The same exact process applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:  (30751, 1447)\n",
      "0  non-unique valued columns.\n",
      "8 missing columns with threshold: 90.\n",
      "43 zero variance columns.\n"
     ]
    }
   ],
   "source": [
    "semi_features = pd.read_csv('../input/features_semi.csv')\n",
    "semi_features = semi_features[semi_features['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "semi_features = feature_selection(semi_features, 90, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_features.to_csv('../input/features_semi_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Features\n",
    "\n",
    "These are the features available in the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pd.read_csv('../input/application_train.csv')\n",
    "fm = fm.sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "fm = pd.get_dummies(fm)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = feature_selection(fm, 90, 0.95)\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.to_csv('../input/features_default_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions from Feature Selection\n",
    "\n",
    "In order to allow us to perform feature selection, we first had to limit the number of rows of data to 10% of the training observations. Then we were able to apply feature selection to reduce the data dimensionality. The final sampled and selected data was saved as `_sample.csv` and can now be used for random search with the Gradient Boosting Machine. There are a number of \"arbitrary\" thresholds in this approach, but by applying the same operations to all 3 datasets, it is hoped that these choices will not affect the integrity of the analysis. At the end of the day, some results are better than no results and these operations will allow us to perform 100 iterations of random search and proceed with the project. \n",
    "\n",
    "The next step is Subsetting Data where we use these results in order to create versions of the feature matrices with all the observations but only the columns identified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting Data\n",
    "\n",
    "Now that we have identified the features we want to keep, we can subset the full data matrices to these features. The `_sample.csv` files are the sampled version (10% of the observations) with the feature selection applied. The `fm` dataframes are the full data. The full data after being subsetted is then saved as `_selected.csv`.\n",
    "\n",
    "The random search is run on the `_sample.csv` files, but the `_selected.csv` files will be used to train and test the final model. These files have all the observations, but only the columns that remained after feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = pd.read_csv('../input/application_train.csv'), pd.read_csv('../input/application_test.csv')\n",
    "test['TARGET'] = np.nan\n",
    "train, test = pd.get_dummies(train).align(pd.get_dummies(test), axis = 1, join = 'inner')\n",
    "\n",
    "sample = pd.read_csv('../input/features_default_sample.csv')\n",
    "fm = train.append(test, ignore_index = True)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = fm[[x for x in sample.columns if x in fm.columns]]\n",
    "fm.to_csv('../input/features_default_selected.csv')\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../input/features_manual_sample.csv')\n",
    "fm = pd.read_csv('../input/features_manual.csv')\n",
    "\n",
    "# One-hot encoding\n",
    "fm = pd.get_dummies(fm)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to the columns in the sample\n",
    "fm = fm[sample.columns]\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Engineering Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sample and full data\n",
    "sample = pd.read_csv('../input/feature_matrix_sample.csv')\n",
    "fm = pd.read_csv('../input/feature_matrix.csv')\n",
    "\n",
    "print(fm.shape)\n",
    "\n",
    "# One hot encoding\n",
    "cat = pd.get_dummies(fm.select_dtypes('object'))\n",
    "\n",
    "# Convert the column types\n",
    "for col in fm:\n",
    "    if fm[col].dtype == 'bool':\n",
    "        fm[col] = fm[col].astype(np.uint8)\n",
    "        \n",
    "# Add the one-hot encoded columns\n",
    "fm = fm.select_dtypes(['number'])\n",
    "fm = pd.concat([fm, cat], axis = 1)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../input/feature_matrix_sample.csv')\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to the columns in the sample\n",
    "fm = fm[[x for x in sample.columns if x in fm]]\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "fm.to_csv('../input/feature_matrix_selected.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Automated Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../input/features_semi_sample.csv')\n",
    "fm = pd.read_csv('../input/features_semi.csv')\n",
    "\n",
    "# One hot encoding\n",
    "fm = pd.get_dummies(fm)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to the columns in sample\n",
    "fm = fm[sample.columns]\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.to_csv('../input/features_semi_selected.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The resulting data can now be used for modeling. The columns have been reduced through feature selection but all of the observations remain. The final datasets will be tested both with the default gradient boosting machine hyperparameters and with the optimal hyperparameters found from random search. \n",
    "\n",
    "The next step is to run random search on these results. This is implemented in the `random_search.py` script in the scripts directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
