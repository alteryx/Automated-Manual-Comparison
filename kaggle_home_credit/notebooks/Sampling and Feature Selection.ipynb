{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Data\n",
    "\n",
    "The purpose of this notebook is to sample 10% of the training observations and then perform feature selection. This is necessary because the large size of the full feature matrices does not allow for efficient feature selection or for a significant number of random search iterations.\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "1. Sample 10% of the training observations randomly\n",
    "2. Convert numeric columns to `np.float32`\n",
    "3. Convert boolean columns to `np.uint8`\n",
    "4. One-hot encode categorical features as necessary\n",
    "5. Remove one of every pair of columns with all duplicated values (1.0 correlation)\n",
    "6. Remove columns with more than 90% missing values\n",
    "7. Remove columns with a single unique value\n",
    "8. Remove one of every pair of columns with abs(correlation) > 0.95\n",
    "\n",
    "The reduced data set will then be saved as `_sample.csv` and can be used for 100 iterations of random search with the Gradient Boosting Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuretools Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = pd.read_csv('../input/feature_matrix.csv', low_memory=False)\n",
    "\n",
    "# Sampling 10% of the original data\n",
    "train = feature_matrix[feature_matrix['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "del feature_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE) not in data\n",
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Closed) not in data\n",
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Active) not in data\n",
      "SUM(bureau_balance.bureau.PREVIOUS_OTHER_LOAN_RATE) not in data\n"
     ]
    }
   ],
   "source": [
    "for col in ['SUM(bureau.PREVIOUS_OTHER_LOAN_RATE)', 'SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Closed)',\n",
    "            'SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Active)', 'SUM(bureau_balance.bureau.PREVIOUS_OTHER_LOAN_RATE)']:\n",
    "    try:\n",
    "        train[col] = train[col].astype(np.float32)\n",
    "    except:\n",
    "        print(f'{col} not in data')\n",
    "    \n",
    "for col in train:\n",
    "    if train[col].dtype == 'bool':\n",
    "        train[col] = train[col].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 2091)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.get_dummies(train)\n",
    "n_features_start = train.shape[1] - 2\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns with duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1816)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, idx, inv, counts = np.unique(train, axis = 1, return_index = True, return_inverse=True, return_counts=True)\n",
    "train = train.iloc[:, idx]\n",
    "n_non_unique_columns = n_features_start - train.shape[1] - 2\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "The threshold is currently set at 90% but could be lowered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1798)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_threshold = 90\n",
    "\n",
    "# Find missing and percentage\n",
    "missing = pd.DataFrame(train.isnull().sum())\n",
    "missing['percent'] = 100 * (missing[0] / train.shape[0])\n",
    "missing.sort_values('percent', ascending = False, inplace = True)\n",
    "\n",
    "# Missing above threshold\n",
    "missing_cols = list(missing[missing['percent'] > missing_threshold].index)\n",
    "n_missing_cols = len(missing_cols)\n",
    "\n",
    "train = train[[x for x in train if x not in missing_cols]]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero variance columns\n",
    "\n",
    "These are any columns with only a single unique value. (`np.nan` does not count as a unique value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1771)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counts = pd.DataFrame(train.nunique()).sort_values(0, ascending = True)\n",
    "zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "train = train[[x for x in train if x not in zero_variance_cols]]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns containing derivations of target\n",
    "\n",
    "This is a special consideration we have to take with the Featuretools data because one of the columns is derived from the `TARGET`. (PERCENTILE transformation works on numeric columns of the data and I accidentally left in the `TARGET` when running the feature synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET\n",
      "PERCENTILE(TARGET)\n"
     ]
    }
   ],
   "source": [
    "for col in train:\n",
    "    if 'TARGET' in col:\n",
    "        print(col)\n",
    "        \n",
    "train.drop(columns = 'PERCENTILE(TARGET)', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Collinear Variables with Correlation Threshold\n",
    "\n",
    "The correlation threshold will be set at 0.95 and one out of every pair of columns that are above this threshold will be removed. The column removed is the one that occurs last in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.95\n",
    "\n",
    "corr_matrix = train.corr()\n",
    "\n",
    "# Extract the upper triangle of the correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# Select the features with correlations above the threshold\n",
    "# Need to use the absolute value\n",
    "to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1042)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[[x for x in train if x not in to_drop]]\n",
    "n_collinear = len(to_drop)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_non_unique_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_zero_variance_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_collinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns removed:  1045\n"
     ]
    }
   ],
   "source": [
    "total_removed = n_non_unique_columns + n_missing_cols + n_zero_variance_cols + n_collinear + 1\n",
    "print('Total columns removed: ', total_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>SUM(installments.DAYS_ENTRY_PAYMENT)</th>\n",
       "      <th>MIN(previous.SUM(installments.DAYS_ENTRY_PAYMENT))</th>\n",
       "      <th>MEAN(previous.SUM(installments.DAYS_ENTRY_PAYMENT))</th>\n",
       "      <th>MAX(previous.SUM(installments.DAYS_ENTRY_PAYMENT))</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>SUM(previous.DAYS_DECISION)</th>\n",
       "      <th>MIN(previous.DAYS_DECISION)</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENTILE(MEAN(bureau.AMT_ANNUITY))</th>\n",
       "      <th>MIN(bureau.PERCENTILE(AMT_ANNUITY))</th>\n",
       "      <th>MEAN(bureau.PERCENTILE(AMT_ANNUITY))</th>\n",
       "      <th>MIN(bureau.NUM_UNIQUE(bureau_balance.STATUS))</th>\n",
       "      <th>MEAN(bureau.NUM_UNIQUE(bureau_balance.STATUS))</th>\n",
       "      <th>MAX(bureau.NUM_UNIQUE(bureau_balance.STATUS))</th>\n",
       "      <th>NUM_UNIQUE(bureau_balance.STATUS)</th>\n",
       "      <th>MIN(bureau.AMT_ANNUITY)</th>\n",
       "      <th>MEAN(bureau.AMT_ANNUITY)</th>\n",
       "      <th>MAX(bureau.AMT_ANNUITY)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83147</th>\n",
       "      <td>-11819.0</td>\n",
       "      <td>-4187.0</td>\n",
       "      <td>-726.0</td>\n",
       "      <td>-726.0</td>\n",
       "      <td>-726.000000</td>\n",
       "      <td>-726.0</td>\n",
       "      <td>-673.0</td>\n",
       "      <td>-601.0</td>\n",
       "      <td>-217.0</td>\n",
       "      <td>-217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354935</th>\n",
       "      <td>-18794.0</td>\n",
       "      <td>-6010.0</td>\n",
       "      <td>-157073.0</td>\n",
       "      <td>-122152.0</td>\n",
       "      <td>-39268.250000</td>\n",
       "      <td>-1026.0</td>\n",
       "      <td>-8164.0</td>\n",
       "      <td>-2201.0</td>\n",
       "      <td>-8634.0</td>\n",
       "      <td>-2648.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64916</th>\n",
       "      <td>-20741.0</td>\n",
       "      <td>-1882.0</td>\n",
       "      <td>-20122.0</td>\n",
       "      <td>-13200.0</td>\n",
       "      <td>-10061.000000</td>\n",
       "      <td>-6922.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4296.0</td>\n",
       "      <td>-2900.0</td>\n",
       "      <td>-2041.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87122</th>\n",
       "      <td>-9881.0</td>\n",
       "      <td>-932.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-185.0</td>\n",
       "      <td>-1076.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280351</th>\n",
       "      <td>-14762.0</td>\n",
       "      <td>-6189.0</td>\n",
       "      <td>-57878.0</td>\n",
       "      <td>-28635.0</td>\n",
       "      <td>-19292.666667</td>\n",
       "      <td>-10697.0</td>\n",
       "      <td>-2370.0</td>\n",
       "      <td>-4187.0</td>\n",
       "      <td>-7679.0</td>\n",
       "      <td>-2745.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661964</td>\n",
       "      <td>0.706131</td>\n",
       "      <td>0.706131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10984.5</td>\n",
       "      <td>10984.5</td>\n",
       "      <td>10984.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1042 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DAYS_BIRTH  DAYS_REGISTRATION  SUM(installments.DAYS_ENTRY_PAYMENT)  \\\n",
       "83147     -11819.0            -4187.0                                -726.0   \n",
       "354935    -18794.0            -6010.0                             -157073.0   \n",
       "64916     -20741.0            -1882.0                              -20122.0   \n",
       "87122      -9881.0             -932.0                                   NaN   \n",
       "280351    -14762.0            -6189.0                              -57878.0   \n",
       "\n",
       "        MIN(previous.SUM(installments.DAYS_ENTRY_PAYMENT))  \\\n",
       "83147                                              -726.0    \n",
       "354935                                          -122152.0    \n",
       "64916                                            -13200.0    \n",
       "87122                                                 NaN    \n",
       "280351                                           -28635.0    \n",
       "\n",
       "        MEAN(previous.SUM(installments.DAYS_ENTRY_PAYMENT))  \\\n",
       "83147                                         -726.000000     \n",
       "354935                                      -39268.250000     \n",
       "64916                                       -10061.000000     \n",
       "87122                                                 NaN     \n",
       "280351                                      -19292.666667     \n",
       "\n",
       "        MAX(previous.SUM(installments.DAYS_ENTRY_PAYMENT))  DAYS_EMPLOYED  \\\n",
       "83147                                              -726.0          -673.0   \n",
       "354935                                            -1026.0         -8164.0   \n",
       "64916                                             -6922.0             NaN   \n",
       "87122                                                 NaN          -185.0   \n",
       "280351                                           -10697.0         -2370.0   \n",
       "\n",
       "        DAYS_ID_PUBLISH  SUM(previous.DAYS_DECISION)  \\\n",
       "83147            -601.0                       -217.0   \n",
       "354935          -2201.0                      -8634.0   \n",
       "64916           -4296.0                      -2900.0   \n",
       "87122           -1076.0                          NaN   \n",
       "280351          -4187.0                      -7679.0   \n",
       "\n",
       "        MIN(previous.DAYS_DECISION)           ...             \\\n",
       "83147                        -217.0           ...              \n",
       "354935                      -2648.0           ...              \n",
       "64916                       -2041.0           ...              \n",
       "87122                           NaN           ...              \n",
       "280351                      -2745.0           ...              \n",
       "\n",
       "        PERCENTILE(MEAN(bureau.AMT_ANNUITY))  \\\n",
       "83147                                    NaN   \n",
       "354935                                   NaN   \n",
       "64916                                    NaN   \n",
       "87122                                    NaN   \n",
       "280351                              0.661964   \n",
       "\n",
       "        MIN(bureau.PERCENTILE(AMT_ANNUITY))  \\\n",
       "83147                                   NaN   \n",
       "354935                                  NaN   \n",
       "64916                                   NaN   \n",
       "87122                                   NaN   \n",
       "280351                             0.706131   \n",
       "\n",
       "        MEAN(bureau.PERCENTILE(AMT_ANNUITY))  \\\n",
       "83147                                    NaN   \n",
       "354935                                   NaN   \n",
       "64916                                    NaN   \n",
       "87122                                    NaN   \n",
       "280351                              0.706131   \n",
       "\n",
       "        MIN(bureau.NUM_UNIQUE(bureau_balance.STATUS))  \\\n",
       "83147                                             NaN   \n",
       "354935                                            NaN   \n",
       "64916                                             NaN   \n",
       "87122                                             NaN   \n",
       "280351                                            3.0   \n",
       "\n",
       "        MEAN(bureau.NUM_UNIQUE(bureau_balance.STATUS))  \\\n",
       "83147                                              NaN   \n",
       "354935                                             NaN   \n",
       "64916                                              NaN   \n",
       "87122                                              NaN   \n",
       "280351                                             3.0   \n",
       "\n",
       "        MAX(bureau.NUM_UNIQUE(bureau_balance.STATUS))  \\\n",
       "83147                                             NaN   \n",
       "354935                                            NaN   \n",
       "64916                                             NaN   \n",
       "87122                                             NaN   \n",
       "280351                                            3.0   \n",
       "\n",
       "        NUM_UNIQUE(bureau_balance.STATUS)  MIN(bureau.AMT_ANNUITY)  \\\n",
       "83147                                 NaN                      NaN   \n",
       "354935                                NaN                      NaN   \n",
       "64916                                 NaN                      NaN   \n",
       "87122                                 NaN                      NaN   \n",
       "280351                                3.0                  10984.5   \n",
       "\n",
       "        MEAN(bureau.AMT_ANNUITY)  MAX(bureau.AMT_ANNUITY)  \n",
       "83147                        NaN                      NaN  \n",
       "354935                       NaN                      NaN  \n",
       "64916                        NaN                      NaN  \n",
       "87122                        NaN                      NaN  \n",
       "280351                   10984.5                  10984.5  \n",
       "\n",
       "[5 rows x 1042 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save sample of Data\n",
    "\n",
    "The resulting data has 10% of the training observations and 1042 columns (1040 of which are features). This data is now ready for random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../input/feature_matrix_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Feature Selection\n",
    "\n",
    "We can refactor the four steps completed above into a single function that applies them in the same sequence to any dataframe. This function will be used on the manual and semi-automated feaures with the same inputs as with the Featuretools features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(feature_matrix, missing_threshold=90, correlation_threshold=0.95):\n",
    "    \"\"\"Feature selection for a dataframe.\"\"\"\n",
    "    \n",
    "    feature_matrix = pd.get_dummies(feature_matrix)\n",
    "    n_features_start = feature_matrix.shape[1]\n",
    "    print('Original shape: ', feature_matrix.shape)\n",
    "\n",
    "    _, idx = np.unique(feature_matrix, axis = 1, return_index = True)\n",
    "    feature_matrix = feature_matrix.iloc[:, idx]\n",
    "    n_non_unique_columns = n_features_start - feature_matrix.shape[1]\n",
    "    print('{}  non-unique valued columns.'.format(n_non_unique_columns))\n",
    "\n",
    "    # Find missing and percentage\n",
    "    missing = pd.DataFrame(feature_matrix.isnull().sum())\n",
    "    missing['percent'] = 100 * (missing[0] / feature_matrix.shape[0])\n",
    "    missing.sort_values('percent', ascending = False, inplace = True)\n",
    "\n",
    "    # Missing above threshold\n",
    "    missing_cols = list(missing[missing['percent'] > missing_threshold].index)\n",
    "    n_missing_cols = len(missing_cols)\n",
    "\n",
    "    # Remove missing columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in missing_cols]]\n",
    "    print('{} missing columns with threshold: {}.'.format(n_missing_cols,\n",
    "                                                                        missing_threshold))\n",
    "    \n",
    "    # Zero variance\n",
    "    unique_counts = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending = True)\n",
    "    zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "    n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "    # Remove zero variance columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in zero_variance_cols]]\n",
    "    print('{} zero variance columns.'.format(n_zero_variance_cols))\n",
    "    \n",
    "    # Correlations\n",
    "    corr_matrix = feature_matrix.corr()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "    # Select the features with correlations above the threshold\n",
    "    # Need to use the absolute value\n",
    "    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "    n_collinear = len(to_drop)\n",
    "    \n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]\n",
    "    print('{} collinear columns removed with threshold: {}.'.format(n_collinear,\n",
    "                                                                          correlation_threshold))\n",
    "    \n",
    "    total_removed = n_non_unique_columns + n_missing_cols + n_zero_variance_cols + n_collinear\n",
    "    \n",
    "    print('Total columns removed: ', total_removed)\n",
    "    print('Shape after feature selection: {}.'.format(feature_matrix.shape))\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Features\n",
    "\n",
    "The process is the same except now we have a function to carry out the feature selection. First we subset to 10% of the training data, and then we apply feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features = pd.read_csv('../input/features_manual.csv')\n",
    "manual_features = manual_features[manual_features['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "manual_features = feature_selection(manual_features, 90, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can save the features for random search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features.to_csv('../input/features_manual_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Automated Features\n",
    "\n",
    "The final feature matrix is that created by what we called __semi-automated__ feature engineering. The same exact process applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_features = pd.read_csv('../input/features_semi.csv')\n",
    "semi_features = semi_features[semi_features['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "semi_features = feature_selection(semi_features, 90, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_features.to_csv('../input/features_semi_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Features\n",
    "\n",
    "These are the features available in the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pd.read_csv('../input/application_train.csv')\n",
    "fm = fm.sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "fm = pd.get_dummies(fm)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = feature_selection(fm, 90, 0.95)\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.to_csv('../input/features_default_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In order to allow us to perform feature selection, we first had to limit the number of rows of data to 10% of the training observations. Then we were able to apply feature selection to reduce the data dimensionality. The final sampled and selected data was saved as `_sample.csv` and can now be used for random search with the Gradient Boosting Machine. There are a number of \"arbitrary\" thresholds in this approach, but by applying the same operations to all 3 datasets, it is hoped that these choices will not affect the integrity of the analysis. At the end of the day, some results are better than no results and these operations will allow us to perform 100 iterations of random search and proceed with the project. \n",
    "\n",
    "The next step is to run random search on these results. This is implemented in the `random_search.py` script in the scripts directory.\n",
    "\n",
    "The next notebook is Subsetting Data where we use these results in order to create versions of the feature matrices with all the observations but only the columns identified in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
